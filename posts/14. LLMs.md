---
title: "Large language models (LLMs)"
date: "2023-09-04"
description: "Subset of traditional machine learning. Models learn from large human-created datasets."
language: "AI"
categories: ["Machine Learning", "AI"]
---

## Generative AI Overview:

- Generative AI mimics human abilities.
- Models learn from large human-created datasets.
- Large language models (LLMs) have billions of parameters.

**Foundation Models:**

- Foundation models vary in parameter size.
- Parameters act as the model's memory.
- More parameters enable advanced tasks.

**Interaction with Language Models:**

- Input to LLMs is called a "prompt."
- Output is a "completion."
- The process is known as "inference."

## LLM use cases and tasks:

Generative AI and LLMs, often associated with chatbots, extend beyond conversations. They excel in various text generation tasks:

- Essay writing from prompts.
- Dialogue summarization.
- Language translation (multiple languages).
- Translating natural language into code (e.g., Python).
- Information retrieval, including named entity recognition.
- Integration with external data sources and APIs for real-world interactions.

These capabilities evolve with model scale and architecture. Larger models offer deeper language understanding, while smaller ones can be fine-tuned for specific tasks, unlocking their full potential for diverse text generation tasks.

**Text Generation Before Transformers:**

- Previous generative algorithms relied on recurrent neural networks (RNNs).
- RNNs faced limitations, especially in predicting with limited context.
- Scaling RNNs demanded substantial computational resources.

**Language Complexity Challenges:**

- Language complexity includes homonyms with multiple meanings.
- Syntactic ambiguity often makes word meaning context-dependent.

## The Transformer Revolution:

- In 2017, the transformer architecture introduced a transformative change.
- Transformers efficiently scaled with GPUs and processed larger datasets.
- They learned word meanings through attention mechanisms.
- This marked a turning point in modern text generation.

**Transformer Architecture: The Key to Text Generation:**

- Transformers, using the transformer architecture, revolutionized text generation.
- They excel in understanding the relevance and context of words in a sentence.
- Transformers apply attention to relationships between all words in a sentence, not just adjacent ones.
- Attention mechanisms allow the model to understand complex language constructs and ambiguities.

**High-Level Overview of Transformer Model:**

- The transformer architecture consists of two parts: the encoder and decoder.
- Inputs are at the bottom, and outputs are at the top.
- Tokenization converts words into numerical tokens.
- Embedding layers map tokens into high-dimensional vectors, capturing word meaning and context.
- Positional encoding preserves word order.
- Self-attention layers analyze relationships between tokens, with multiple attention heads for different language aspects.
- The model's multi-headed self-attention allows it to learn various language aspects.
- Outputs are processed through a fully-connected feed-forward network.
- The final softmax layer generates probability scores for each word in the vocabulary.

Transformers have revolutionized text generation by understanding language complexity and capturing contextual dependencies, making them a cornerstone of modern NLP.

## Generating Text with Transformers:

- This section provides an overview of how text generation works within the transformer architecture.
- It uses a translation task as an example, where a transformer model translates a French phrase into English.
- Tokenization converts input words into tokens.
- Tokens are passed to the encoder, go through multi-headed attention layers, and enter a feed-forward network.
- Encoder output represents the input's structure and meaning.
- The decoder, triggered by a start-of-sequence token, predicts the next token based on encoder context.
- Looping continues until an end-of-sequence token is predicted.
- The final sequence of tokens is detokenized into words to produce the output.
- The softmax layer is used for token prediction, influencing the creativity of the generated text.
- Transformers consist of encoder and decoder components for various tasks.

**Variations of Transformers:**

- Encoder-only models, like BERT, are suitable for classification tasks and sequence-to-sequence models with equal input and output lengths.
- Encoder-decoder models, such as T5, handle sequence-to-sequence tasks with varying input and output lengths, and they can be used for general text generation.
- Decoder-only models, like GPT, are versatile and widely used, capable of various tasks like text generation, summarization, and more.

Understanding these transformer variations helps navigate the world of transformer models and model documentation.

**Prompt Engineering:**

- Interaction with transformer models primarily involves creating prompts using written words, not code.
- You don't need in-depth knowledge of the underlying architecture for prompt engineering.
- Prompt engineering will be explored further in the next part of the course.

The goal is to provide enough background knowledge to distinguish between transformer models and understand how to work with them through natural language prompts.

**Transformers: Attention is All You Need**

- "Attention is All You Need" is a 2017 research paper by Google researchers.
- It introduced the Transformer model, revolutionizing natural language processing (NLP) and inspiring models like GPT and PaLM.
- The Transformer architecture replaced RNNs and CNNs with a self-attention mechanism.
- Self-attention captures long-term dependencies and enables efficient parallel computation.
- The model achieved state-of-the-art performance in machine translation tasks.
- The architecture includes encoder and decoder components with multi-head self-attention and feed-forward layers.
- Residual connections and layer normalization aid in training and prevent overfitting.
- Positional encoding encodes token positions, allowing sequence order capture without recurrence or convolution.
- Read the original Transformers paper [here](https://arxiv.org/abs/1706.03762).

## Prompt Engineering and In-Context Learning

- The text given to the model is called the prompt.
- Generating text is called inference, and the output is the completion.
- The context window is the available text memory for the prompt.
- Improving the prompt is known as prompt engineering.
- Including examples in the prompt is called in-context learning.
  1. Zero-shot inference works well for large models but not smaller ones.
  2. One-shot inference involves adding a single example in the prompt.
  3. Few-shot inference uses multiple examples for better understanding.
- Be mindful of the context window's limit for in-context learning.
- If the model struggles, consider fine-tuning for specific tasks.
- Model performance varies with scale; larger models excel in zero-shot tasks.
- Smaller models are task-specific but may suit certain use cases.
- Experiment with configuration settings to influence completion style.
- More details on configuration settings will be explored in the next video.

## Generative Configration

**Controlling Model Output During Inference**

- Configuration parameters at inference time allow control over model output.
- These parameters differ from training parameters and affect how the model generates text.

**Max New Tokens**

- Limits the number of tokens generated in the completion.
- Acts as a cap on the selection process.
- The output may be shorter if other stop conditions are met.

**Greedy Decoding**

- Default method where the model chooses the most probable word.
- May lead to repeated words or sequences.

**Random Sampling**

- Introduces variability by selecting words randomly based on probability.
- Reduces repetition but may produce less sensible text.
- Sometimes needs to be explicitly enabled.

**Top K and Top P Sampling**

- Limit random sampling by selecting from the top K most probable tokens.
- Or limit it based on the total probability not exceeding P.
- Helps maintain sensible output while adding variability.

**Temperature Parameter**

- Influences the shape of the probability distribution for the next token.
- Lower values (less than 1) lead to more focused output.
- Higher values (greater than 1) result in broader, more random output.
- A value of 1 keeps the default distribution.

**Experimenting with Configuration Parameters**

- Adjust these parameters to influence completion style.
- Temperature, top K, and top P affect randomness and variability.

## Generative AI Project Life Cycle

1. **Define Scope**

   - Specify the intended function and scope of your language model.
   - Consider whether the model needs to perform various tasks or a specific task.
   - Be specific to optimize performance and compute cost.

2. **Select Model**

   - Decide whether to train your own model or use an existing base model.
   - Most projects start with an existing model, but training from scratch may be necessary in some cases.

3. **Assess & Improve Model**

   - Assess the model's performance for your application.
   - Utilize in-context learning (examples within prompts) to improve performance.
   - Fine-tune the model if necessary, a process covered in detail in Week 2.

4. **Adapt & Align**

   - Ensure the model behaves ethically and aligns with human preferences.
   - Learn about reinforcement learning with human feedback in Week 3.
   - Evaluate model outputs, iterate, and fine-tune to meet performance goals.

5. **Deploy & Integrate**

   - Deploy the model into your infrastructure.
   - Optimize the model for deployment and resource utilization.
   - Consider additional infrastructure requirements.

6. **Address Limitations**
   - Address fundamental limitations of LLMs, such as inventing information and limited reasoning abilities.
   - Learn techniques to overcome these limitations in the final part of the course.

This life cycle provides a structured approach to developing and deploying LLM-powered applications.
